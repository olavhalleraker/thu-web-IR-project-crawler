# ğŸ” BiasSearch Crawler - Tsinghua Web Information Retrieval Project

This repository (`thu-web-IR-project-crawler`) provides the web crawling component for the **BiasSearch** project, an AI-powered news article search engine that retrieves and classifies articles **in favor**, **against**, or **neutral** with respect to the user's query. It is developed for the **Web Information Retrieval** course at **Tsinghua University**.

The repository handles:

- **Automated crawling** of news websites through their sitemaps to discover articles
- **Metadata extraction** including article URLs, titles, summaries, image URLs and publication dates
- **Data preparation** by storing articles in structured JSON format for the retrieval system
- **Language filtering** (optional) to maintain English-only content

The crawled articles are used by the search API, in the repository (`thu-web-IR-project-api`), which processes them for retrieval and classification
so frontend web application, in the repository (in a separate repository) ultimately displays them to users based on their query


---

## ğŸ“ Repository Structure


```
thu-web-IR-project-crawler
â”‚
â”œâ”€â”€ articles/                   # Folder containing crawled JSON article data
â”‚
â”œâ”€â”€ crawler/                    # Scrapy project folder
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py                # Placeholder for item definitions (not used)
â”‚   â”œâ”€â”€ middlewares.py          # Optional custom middleware (default behavior)
â”‚   â”œâ”€â”€ pipelines.py            # Optional processing pipeline (default behavior)
â”‚   â”œâ”€â”€ settings.py             # Scrapy project settings
â”‚   â”œâ”€â”€ json_unifier.py         # Utility to merge all individual JSONs into one
â”‚   â””â”€â”€ spiders/                # Spider folder (used by scrapy framework)
â”‚       â”œâ”€â”€ news_crawler.py     # Main Scrapy spider for crawling news articles
â”‚
â”œâ”€â”€ articles_database.json      # JSON output of json_unifier.py
â”œâ”€â”€ scrapy.cfg                  # Scrapy configuration file
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # You're reading this!
```
---

## ğŸ”§ Usage Instructions

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```
### 2 Generate a scrapy spider based on a template

```bash
scrapy genspider -t [type] [name] [domain]
```

### 3. Run spiders in the `spiders` folder
```bash
scrapy crawl [crawler name] -o [file].[filetype]
```

### 4. Run `news_cralwer.py` spider
```bash
scrapy runspider news_crawler.py -a start_urls=https://example.com/sitemap.xml -a is_sitemap_index=True -a enable_lang_detection=True -o articles/example.json
```

**Configuration Flags:**
- `start_urls`: Initial URL to start crawling from
- `is_sitemap_index`: Set to `True` when crawling a sitemap index
- `enable_lang_detection`: Set to `True` to filter non-English content
- `-o`: Output file to save crawled data
---
## ğŸ”‘ Key Components

### 1. News Crawler ğŸ•·ï¸
Implemented in `search.py` The primary spider implementation that:
- Handles both sitemap indexes and individual sitemaps
- Extracts article URLs and metadata
- Implements english language detection (optional)
- Respects robots.txt rules


### 2. JSON unifier ğŸ”—
Implemented in `search.py`, utility script that:
- Combines all individual article JSON files from the `articles/` directory
- Creates a single unified JSON database (`articles_database.json`)
- Maintains proper JSON formatting

### 3. Scrapy Configuration Files âš™ï¸
- `settings.py`: Configures crawler behavior (user agents, throttling, etc.)
- `middlewares.py`: Custom middleware implementations
- `pipelines.py`: Item processing pipelines (currently minimal)
- `items.py`: Data structure definitions (currently empty)

---
Olav Larsen Halleraker  
Guillermo Rodrigo PÃ©rez  
Project for Web Information Retrieval â€” Tsinghua University 2024/2025